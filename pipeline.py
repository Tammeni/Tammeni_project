# -*- coding: utf-8 -*-
"""Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11LADGH-fZqWW8xeb7jfp_75ucK4N8NMl

# Imports
"""

!pip install jais transformers datasets torch evaluate regex accelerate xgboost

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score, learning_curve
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, ElasticNet
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from imblearn.over_sampling import SMOTE
import xgboost as xgb
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem.isri import ISRIStemmer
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel
from huggingface_hub import login

import regex as reg
import re
import ast
import torch

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

arabic_stopwords = set(stopwords.words('arabic'))
stemmer = ISRIStemmer()
label_encoder = LabelEncoder()
AnxEncoder = LabelEncoder()
DepEncoder = LabelEncoder()
smote = SMOTE(random_state=42)
Sbert = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')

"""# JAIS"""

!accelerate config
# 0
# 0
# NO
# yes
# 0
# NO
# NO
# all
# NO
# 1

login(token="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")
# 8 bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
)
tokenizer = AutoTokenizer.from_pretrained("inceptionai/jais-13b-chat", padding_side='left')

model = AutoModelForCausalLM.from_pretrained(
    "inceptionai/jais-13b-chat",
    device_map="auto",
    trust_remote_code=True
)

def translate_to_fusha(text):
    try:
        # Skip empty/invalid strings
        if not isinstance(text, str) or text.strip() == "":
            return ""

        prompt = f"""حول الجملة التالية من اللهجة العامية إلى اللغة العربية الفصحى:\n"{text}"\nالنتيجة:"""
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.7)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Cleaning
        del inputs, outputs
        torch.cuda.empty_cache()

        return response.split("النتيجة:")[-1].strip()

    except Exception as e:
        print(f"Error translating: {text[:30]}... — {e}")
        torch.cuda.empty_cache()
        return ""

def clean_text(text):
  cleaned = re.sub(r'[\'\"\n\d,;.،؛.؟]', ' ', text)
  cleaned = re.sub(r'[ؐ-ًؚٟ]*', '', cleaned)
  cleaned = re.sub(r'\s{2,}', ' ', cleaned)

  emoji_pattern = re.compile("["
  u"\U0001F600-\U0001F64F" # emoticons
  u"\U0001F300-\U0001F5FF" # symbols
  u"\U0001F680-\U0001F6FF" # transport/map symbols
  u"\U0001F1E0-\U0001F1FF" # flags
  u"\U00002702-\U000027B0"
  u"\U000024C2-\U0001F251"
  "]+", flags=re.UNICODE)
  cleaned = emoji_pattern.sub(r'', cleaned)
  cleaned = re.sub(r'[\u064B-\u0652]', '', cleaned) # tashkeel removal
  # Normalization
  cleaned = re.sub(r'[إأآا]', 'ا', cleaned)
  cleaned = cleaned.replace('ة', 'ه')
  cleaned = cleaned.replace('ى', 'ي')
  cleaned = cleaned.replace("ؤ", "و")
  cleaned = cleaned.replace("ئ", "ي")
  return cleaned.strip()

def clean_and_stem_arabic(text):
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
    tokens = text.split()
    filtered = [t for t in tokens if t not in arabic_stopwords]
    stemmed = [stemmer.stem(word) for word in filtered]
    return " ".join(stemmed)

def encode_Sbert(questions,answers):
  questions = [clean_text(text) for text in questions]
  questions = [clean_and_stem_arabic(text) for text in questions]
  question_embeddings = Sbert.encode(questions, convert_to_tensor=True,normalize_embeddings=True)
  similarities = []
  for _, answer in answers.iterrows():
    answer_embeddings = Sbert.encode(answer.tolist(), convert_to_tensor=True, normalize_embeddings=True)
    row_similarities = cos_sim(question_embeddings, answer_embeddings).diagonal()
    similarities.append(row_similarities.tolist())
  df = pd.DataFrame(similarities, columns=[f"Q{i+1}_sim" for i in range(len(questions))])
  return df

def get_score(model, X_test):
  return model.predict_proba(X_test)

def ConfusionMatrix(y_test, y_pred):
  cm = confusion_matrix(y_test, y_pred)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  disp.plot(cmap=plt.cm.Blues)
  plt.show()

def LearningCurve(model, X_train, y_train):
  train_sizes, train_scores, val_scores = learning_curve(
  model, X_train, y_train, cv=5, n_jobs=1, train_sizes=np.linspace(0.1, 1.0, 10)
   )
  # mean and standard deviation for training and validation scores
  train_mean = np.mean(train_scores, axis=1)
  train_std = np.std(train_scores, axis=1)
  val_mean = np.mean(val_scores, axis=1)
  val_std = np.std(val_scores, axis=1)

  # learning curve
  plt.figure(figsize=(10, 6))
  plt.plot(train_sizes, train_mean, label='Training score', color='blue')
  plt.plot(train_sizes, val_mean, label='Validation score', color='red')

  plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)
  plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color='red', alpha=0.2)

  plt.title('Learning Curve for RandomForestClassifier')
  plt.xlabel('Training Set Size')
  plt.ylabel('Accuracy')
  plt.legend(loc='best')
  plt.grid(True)
  plt.show()

def RFC(X,y, best_params):
  # best_params ={'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4}
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  # best_params = Grid_search(X,y)
  rfc = RandomForestClassifier(**best_params)
  rfc.fit(X_train, y_train)
  y_pred = rfc.predict(X_test)
  print("Accuracy:", accuracy_score(y_test, y_pred))
  print("Classification Report:\n", classification_report(y_test, y_pred))
  ConfusionMatrix(y_test, y_pred)
  LearningCurve(rfc, X_train, y_train)
  return rfc, get_score(rfc, X_test)

def up_sample(X, Y):
  return smote.fit_resample(X, Y)

df = pd.read_csv('/content/df_trans_final.csv')
df = df.iloc[::, 1:]
df = df.sample(frac=1, random_state=42).reset_index(drop=True)
df.iloc[::,:6] = df.iloc[::,:6].astype(str).applymap(clean_text)
df.iloc[::,:6] = df.iloc[::,:6].astype(str).applymap(clean_and_stem_arabic)

df = df[~df['Diagnosis'].apply(lambda x: "Another Disorder" in x)]
# depression
df_dep = df.iloc[::,[0,1,2,6,7,8]]
df_dep = df_dep[~df_dep['Diagnosis'].apply(lambda x: "Anxiety" in x)].reset_index(drop=True)
df_dep['Diagnosis'] = df_dep['Diagnosis'].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)
df_dep['Diagnosis'] = DepEncoder.fit_transform(df_dep['Diagnosis'])
# anxiety
df_anx = df.iloc[::,2:]
df_anx = df_anx[~df_anx['Diagnosis'].apply(lambda x: "Depression" in x)].reset_index(drop=True)
df_anx['Diagnosis'] = df_anx['Diagnosis'].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)
df_anx = df_anx[~df_anx['Diagnosis'].apply(lambda x: 'Anxiety' in x and 'Healthy' in x)].reset_index(drop=True)
df_anx['Diagnosis'] = AnxEncoder.fit_transform(df_anx['Diagnosis'])

# best_params = {'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 0.3, 'ccp_alpha': 0.01}
best_params ={'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_samples': 0.6, 'max_leaf_nodes': 10, 'max_features': None, 'max_depth': 5, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': 0.05, 'bootstrap': True}

questions_dep = df_dep.columns.to_list()[:3]
answers_dep = df_dep[questions_dep]
X = encode_Sbert(questions_dep, answers_dep)
y = df_dep['Diagnosis']
X, y = up_sample(X, y)
rfc_dep, RFCDepScore = RFC(X,y,best_params)

best_params = {'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_leaf_nodes': 10, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'entropy', 'class_weight': None, 'ccp_alpha': 0.01, 'bootstrap': True}

questions_anx = df_anx.columns.to_list()[:4]
answers_anx = df_anx[questions_anx]
X = encode_Sbert(questions_anx, answers_anx)
y = df_anx['Diagnosis']
X, y = up_sample(X, y)
rfc_anx, RFCAnxScore = RFC(X,y,best_params)

"""# Assuming we have user input in ***df_user***

**Define df_user for code below:**
"""

# translation
translated_df = pd.DataFrame(index=df_user.index, columns=df_user.columns[2:-1])

for i, row in enumerate(df_user.index):
    for col in df_user.columns[2:8]:

        output = translate_to_fusha(df_user.at[row, col])
        print(output)
        translated_df.at[row, col] = output
translated_df.iloc[::,:6] = translated_df.iloc[::,:6].astype(str).applymap(clean_text)
translated_df.iloc[::,:6] = translated_df.iloc[::,:6].astype(str).applymap(clean_and_stem_arabic)

questions = translated_df.columns.to_list()[:4]
answers_anx = translated_df[questions_anx]
X = encode_Sbert(questions_anx, answers_anx)

print("predicted class for anxiet model: ", rfc_anx.predict(X))
score_anx = get_score(rfc_anx, X)
print("probability for each class for each class in anxiety model: ", score_anx)

print("predicted class for anxiet model: ", rfc_dep.predict(X))
score_dep = get_score(rfc_dep, X)
print("probability for each class for each class in anxiety model: ", score_dep)

from google.colab import files
import joblib

joblib.dump(rfc_dep, 'rfc_dep.pkl')

files.download("rfc_dep.pkl")

from google.colab import files
import joblib

joblib.dump(rfc_anx, 'rfc_anx.pkl')

files.download("rfc_anx.pkl")