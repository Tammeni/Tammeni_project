# -*- coding: utf-8 -*-
"""pipeline_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C_nskj4ptY6GjYBzzhpxIheEkI7AoI0G
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score, learning_curve
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, ElasticNet
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from imblearn.over_sampling import SMOTE
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem.isri import ISRIStemmer
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim

import regex as reg
import re
import ast
import torch

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)

from sklearn.preprocessing import LabelEncoder

DepEncoder = LabelEncoder()
AnxEncoder = LabelEncoder()

from sentence_transformers import SentenceTransformer

Sbert = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def translate_to_fusha(text):
    """
    JAIS removed – return original text unchanged.
    """
    return text

import pandas as pd

df = pd.read_csv('/content/DATA-FORM - Form Responses 1 (1).csv')
cols_to_drop = [3,4,5,6,7,-1]
df = df.drop(df.columns[cols_to_drop], axis=1)

# This grabs rows 0–50 and columns 2 through 7
df = df.iloc[:51, 2:8]

translated_df = pd.DataFrame(index=df.index, columns=df.columns)

for i, row in enumerate(df.index):
    for col in df.columns:
        output = translate_to_fusha(df.at[row, col])  # currently does nothing
        print(output)
        translated_df.at[row, col] = output

translated_df.to_excel('new.xlsx', index=False)

from google.colab import files
files.download('new.xlsx')

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.isri import ISRIStemmer

# Download NLTK resources once
nltk.download('stopwords')

# Initialize Arabic stopwords and stemmer
arabic_stopwords = set(stopwords.words('arabic'))
stemmer = ISRIStemmer()

def clean_text(text):
    """
    Cleans Arabic text by removing punctuation, tashkeel, emojis,
    normalizing letters, and collapsing spaces.
    """
    cleaned = re.sub(r'[\'\"\n\d,;.،؛.؟]', ' ', text)
    cleaned = re.sub(r'[ؐ-ًؚٟ]*', '', cleaned)
    cleaned = re.sub(r'\s{2,}', ' ', cleaned)

    # Remove emojis
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    cleaned = emoji_pattern.sub(r'', cleaned)

    # Remove tashkeel
    cleaned = re.sub(r'[\u064B-\u0652]', '', cleaned)

    # Normalize Arabic characters
    cleaned = re.sub(r'[إأآا]', 'ا', cleaned)
    cleaned = cleaned.replace('ة', 'ه').replace('ى', 'ي')
    cleaned = cleaned.replace("ؤ", "و").replace("ئ", "ي")

    return cleaned.strip()

def clean_and_stem_arabic(text):
    """
    Removes non-Arabic characters, filters out stopwords, and applies stemming.
    """
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
    tokens = text.split()
    filtered = [t for t in tokens if t not in arabic_stopwords]
    stemmed = [stemmer.stem(word) for word in filtered]
    return " ".join(stemmed)

def encode_Sbert(questions,answers):
  questions = [clean_text(text) for text in questions]
  questions = [clean_and_stem_arabic(text) for text in questions]
  question_embeddings = Sbert.encode(questions, convert_to_tensor=True,normalize_embeddings=True)
  similarities = []
  for _, answer in answers.iterrows():
    answer_embeddings = Sbert.encode(answer.tolist(), convert_to_tensor=True, normalize_embeddings=True)
    row_similarities = cos_sim(question_embeddings, answer_embeddings).diagonal()
    similarities.append(row_similarities.tolist())
  df = pd.DataFrame(similarities, columns=[f"Q{i+1}_sim" for i in range(len(questions))])
  return df

def get_score(model, X_test):
  return model.predict_proba(X_test)

def ConfusionMatrix(y_test, y_pred):
  cm = confusion_matrix(y_test, y_pred)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  disp.plot(cmap=plt.cm.Blues)
  plt.show()

def LearningCurve(model, X_train, y_train):
  train_sizes, train_scores, val_scores = learning_curve(
  model, X_train, y_train, cv=5, n_jobs=1, train_sizes=np.linspace(0.1, 1.0, 10)
   )
  # mean and standard deviation for training and validation scores
  train_mean = np.mean(train_scores, axis=1)
  train_std = np.std(train_scores, axis=1)
  val_mean = np.mean(val_scores, axis=1)
  val_std = np.std(val_scores, axis=1)

  # learning curve
  plt.figure(figsize=(10, 6))
  plt.plot(train_sizes, train_mean, label='Training score', color='blue')
  plt.plot(train_sizes, val_mean, label='Validation score', color='red')

  plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)
  plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color='red', alpha=0.2)

  plt.title('Learning Curve for RandomForestClassifier')
  plt.xlabel('Training Set Size')
  plt.ylabel('Accuracy')
  plt.legend(loc='best')
  plt.grid(True)
  plt.show()

def up_sample(X, Y):
  return smote.fit_resample(X, Y)

df = pd.read_csv('/content/df_trans_final (1).csv')
df = df.iloc[::, 1:]
df = df.sample(frac=1, random_state=42).reset_index(drop=True)
df.iloc[::,:6] = df.iloc[::,:6].astype(str).applymap(clean_text)
df.iloc[::,:6] = df.iloc[::,:6].astype(str).applymap(clean_and_stem_arabic)

df = df[~df['Diagnosis'].apply(lambda x: "Another Disorder" in x)]
# depression
df_dep = df.iloc[::,[0,1,2,6,7,8]]
df_dep = df_dep[~df_dep['Diagnosis'].apply(lambda x: "Anxiety" in x)].reset_index(drop=True)
df_dep['Diagnosis'] = df_dep['Diagnosis'].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)
df_dep['Diagnosis'] = DepEncoder.fit_transform(df_dep['Diagnosis'])
# anxiety
df_anx = df.iloc[::,2:]
df_anx = df_anx[~df_anx['Diagnosis'].apply(lambda x: "Depression" in x)].reset_index(drop=True)
df_anx['Diagnosis'] = df_anx['Diagnosis'].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)
df_anx = df_anx[~df_anx['Diagnosis'].apply(lambda x: 'Anxiety' in x and 'Healthy' in x)].reset_index(drop=True)
df_anx['Diagnosis'] = AnxEncoder.fit_transform(df_anx['Diagnosis'])

# Import RandomForestClassifier if not already imported
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def RFC(X, y, best_params):
  """
  Trains a RandomForestClassifier model and returns the trained model and its accuracy score.
  """
  # Split data into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

  # Initialize RandomForestClassifier with best parameters
  model = RandomForestClassifier(**best_params)

  # Train the model
  model.fit(X_train, y_train)

  # Predict on the test set
  y_pred = model.predict(X_test)

  # Calculate accuracy score
  score = accuracy_score(y_test, y_pred)

  return model, score

# best_params = {'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 0.3, 'ccp_alpha': 0.01}
best_params ={'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_samples': 0.6, 'max_leaf_nodes': 10, 'max_features': None, 'max_depth': 5, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': 0.05, 'bootstrap': True}

questions_dep = df_dep.columns.to_list()[:3]
answers_dep = df_dep[questions_dep]
X = encode_Sbert(questions_dep, answers_dep)
y = df_dep['Diagnosis']
X, y = up_sample(X, y)
rfc_dep, RFCDepScore = RFC(X,y,best_params)

best_params = {'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_leaf_nodes': 10, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'entropy', 'class_weight': None, 'ccp_alpha': 0.01, 'bootstrap': True}

questions_anx = df_anx.columns.to_list()[:4]
answers_anx = df_anx[questions_anx]
X = encode_Sbert(questions_anx, answers_anx)
y = df_anx['Diagnosis']
X, y = up_sample(X, y)
rfc_anx, RFCAnxScore = RFC(X,y,best_params)

# Step 1: Initialize translated DataFrame with same shape as answers
translated_df = pd.DataFrame(index=df.index, columns=df.columns[2:8])

# Step 2: Translate each answer (currently a no-op if translate_to_fusha just returns text)
for i, row in df.iterrows():
    for col in df.columns[2:8]:
        text = df.at[i, col]
        translated = translate_to_fusha(text)
        translated_df.at[i, col] = translated
        print(translated)

# Step 3: Clean and stem answers
translated_df = translated_df.astype(str).applymap(clean_text)
translated_df = translated_df.astype(str).applymap(clean_and_stem_arabic)

questions = translated_df.columns.to_list()[:4]
answers_anx = translated_df[questions_anx]
X = encode_Sbert(questions_anx, answers_anx)

print("predicted class for anxiet model: ", rfc_anx.predict(X))
score_anx = get_score(rfc_anx, X)
print("probability for each class for each class in anxiety model: ", score_anx)

print(translated_df.columns.tolist())

# Rename to simpler Arabic IDs for easier access
translated_df.columns = ["س1", "س2", "س3", "س4", "age", "gender"]

questions_dep = ["س1", "س2", "س3"]
answers_df = translated_df[questions_dep]
X = encode_Sbert(questions_dep, answers_df)

print("predicted class for anxiet model: ", rfc_dep.predict(X))
score_dep = get_score(rfc_dep, X)
print("probability for each class for each class in anxiety model: ", score_dep)

from google.colab import files
import joblib

joblib.dump(rfc_dep, 'rfc_dep.pkl')

files.download("rfc_dep.pkl")

from google.colab import files
import joblib

joblib.dump(rfc_anx, 'rfc_anx.pkl')

files.download("rfc_anx.pkl")

